{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libaries\n",
    "### Modules needed for NLP\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from textblob import TextBlob\n",
    "#Google modules\n",
    "import httplib2\n",
    "from apiclient import discovery\n",
    "from google.oauth2 import service_account\n",
    "import pygsheets\n",
    "\n",
    "# from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "# def some_job():\n",
    "#     print(\"Hourly sheet update\")\n",
    "\n",
    "# scheduler = BlockingScheduler()\n",
    "# scheduler.add_job(some_job, 'interval', hours=1)\n",
    "# scheduler.start()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorize API's\n",
    "#Authorize Twitter API\n",
    "f = open('/home/pi/twitter_api_creds.json')\n",
    "creds = json.load(f)\n",
    "consumer_key = creds['consumer_key']\n",
    "consumer_secret = creds['consumer_secret']\n",
    "access_token = creds['access_token']\n",
    "access_token_secret = creds['access_token_secret']\n",
    "f.close()\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "#Authorize Google's API\n",
    "scopes = [\"https://www.googleapis.com/auth/drive\", \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "secret_file = '/home/pi/client_secret.json'\n",
    "gc = pygsheets.authorize(service_file='/home/pi/client_secret.json')\n",
    "\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(secret_file, scopes=scopes)\n",
    "service = discovery.build('sheets', 'v4', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter Functions\n",
    "#Calls on clean() for text cleanup then removes the URL\n",
    "def remove_url(txt):\n",
    "    #Call on clean to clean text first\n",
    "    txt = clean(txt)\n",
    "    #removes URL\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "#Cleans up the text like newlines before url is removed.\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"â€” \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter functions\n",
    "#Returns DF of list of cleaned tweets for sentiment analysis\n",
    "#Calls on clean_url() in above block to clean and remove URL\n",
    "#Takes in user inputs as parameters to search for tweets\n",
    "#Can also be used alone if the terms are hardcoded; for testing\n",
    "def extract_tweets_configed(search_term, opt, num, date_ans):\n",
    "    if (opt == 'y'):\n",
    "        search_term = search_term + \" -filter:retweets\"\n",
    "    tweets = tw.Cursor(api.search,\n",
    "                   q=search_term,\n",
    "                   lang=\"en\",\n",
    "                   since=date_ans).items(num)\n",
    "    # Remove URLs by calling on functions in block above\n",
    "    tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]\n",
    "    # Create textblob objects of the tweets\n",
    "    sentiment_objects = [TextBlob(tweet) for tweet in tweets_no_urls]\n",
    "    # Get sentiment values\n",
    "    sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "    #put into dataframe\n",
    "    sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"tweet\"])\n",
    "    #remove neutral sentiment values\n",
    "    #sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "    return sentiment_df\n",
    "#Gets the user inputs to pass to extract_tweets_configed\n",
    "#Prompts for search term, retweet filter option, num to extract and date since.\n",
    "def user_inputs():\n",
    "    # Valid final string ex. \"#climate+change -filter:retweets\"\n",
    "    print(\"Enter tweet search term(s): \")\n",
    "    print(\"E.g. ''#climatechange' should be entered as: #climate+change\")\n",
    "    search_term = input()\n",
    "    #Prompt for retweet filter\n",
    "    opt = input(\"Filter out retweets? Enter as y/n: \")\n",
    "#     if (opt == 'y'):\n",
    "#         search_term = search_term + \" -filter:retweets\"\n",
    "    #prompt for number of tweets to be extracted    \n",
    "    num = input(\"How many tweets to be extracted? Enter as int: \")\n",
    "    num = int(num)\n",
    "    #prompt for date since in YYYY-MM-DD or default to '2018-11-01'\n",
    "    date = '2018-11-01'\n",
    "    date_ans = input(\"Since what date? Enter as: YYYY-MM-DD or 'n' for default of 2018-11-01: \")\n",
    "    if (date_ans == 'n'):\n",
    "        date_ans = date\n",
    "    df = extract_tweets_configed(search_term, opt, num, date_ans)\n",
    "    return df\n",
    "#Calls on user_inputs and returns a cleaned dataframe of tweets ready for sentiment analysis.\n",
    "def extract_tweets():\n",
    "    return user_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set up search for UN SDG user names\n",
    "def extract_user_tweets_configed(user_name, retweet):\n",
    "    tweets = api.user_timeline(screen_name=user_name, \n",
    "                               # 200 is the maximum allowed count\n",
    "                               count=200,\n",
    "                               #Keep retweets \n",
    "                               include_rts = retweet,\n",
    "                               # Necessary to keep full_text \n",
    "                               # otherwise only the first 140 words are extracted\n",
    "                               #eet_mode = 'extended'\n",
    "                               )\n",
    "    # Remove URLs by calling on functions in block above\n",
    "    tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]\n",
    "    # Create textblob objects of the tweets\n",
    "    sentiment_objects = [TextBlob(tweet) for tweet in tweets_no_urls]\n",
    "    # Get sentiment values\n",
    "    sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "    #put into dataframe\n",
    "    sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"tweet\"])\n",
    "    #remove neutral sentiment values\n",
    "    #sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "    return sentiment_df\n",
    "#Extract tweets from UN accounts. \n",
    "def user_extract():\n",
    "    #UN SDG goals account names array\n",
    "    UN_accts = ['GlobalGoalsUN', 'UNDP']\n",
    "    opt = input(\"Include user's retweets? Enter as y/n: \")\n",
    "    if(opt == 'y'):\n",
    "        retweet = True\n",
    "    if(opt == 'n'):\n",
    "        retweet = False\n",
    "    for i in range(0, len(UN_accts)):\n",
    "        df_user = extract_user_tweets_configed(UN_accts[i], retweet)\n",
    "        if (i == 0):\n",
    "            df_UN = df_user\n",
    "        if (i > 0):\n",
    "            df_UN = df_UN.append(df_user, sort = False)\n",
    "    return df_UN\n",
    "# df = user_extract()\n",
    "# df = df[df.polarity != 0]\n",
    "#df_user.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out retweets? Enter as y/n: y\n",
      "How many tweets to be extracted? Enter as int: 1000\n",
      "Since what date? Enter as: YYYY-MM-DD or 'n' for default of 2018-11-01: n\n",
      "Df uploaded\n",
      "Df uploaded\n"
     ]
    }
   ],
   "source": [
    "## Search for all 17 UN SDG's and UN SDG search terms itself\n",
    "## \n",
    "def topic_extract():\n",
    "    #topics array, used: https://ritetag.com/best-hashtags-for/sdgs\n",
    "    search_SDGs = ['#GlobalGoals', '#climate+change', '#sdgs',\n",
    "                  '#sdg', '#development', '#agenda', '#youth',\n",
    "                  \"#sustainability\", '#impact', '#water', '#education',\n",
    "                  '#project', 'fgm', '#health', \n",
    "                   '#sustainable+development', '#sustainable+development+goals',\n",
    "                  '#development+goals', '#learning', '#2020+agenda', '#zeroplastic']\n",
    "    opt = input(\"Filter out retweets? Enter as y/n: \")\n",
    "    #prompt for number of tweets to be extracted    \n",
    "    num = input(\"How many tweets to be extracted? Enter as int: \")\n",
    "    num = int(num)\n",
    "    #prompt for date since in YYYY-MM-DD or default to '2018-11-01'\n",
    "    date = '2018-11-01'\n",
    "    date_ans = input(\"Since what date? Enter as: YYYY-MM-DD or 'n' for default of 2018-11-01: \")\n",
    "    if (date_ans == 'n'):\n",
    "        date_ans = date\n",
    "    for i in range(0, len(search_SDGs)):\n",
    "        search_term = search_SDGs[i]\n",
    "        df_UN = extract_tweets_configed(search_term, opt, num, date_ans)\n",
    "        if (i == 0):\n",
    "            loc = 1\n",
    "        if (i > 0):\n",
    "            loc = loc + 2\n",
    "        df_UN = df_UN[df_UN.polarity != 0]\n",
    "\n",
    "        google_df_upload(df_UN, loc)\n",
    " #   return df_UN\n",
    "#Uploads dataframes to google sheets\n",
    "#Populates two columns at a time for sentiment analysis\n",
    "def google_df_upload(df,loc):\n",
    "    # spreadsheet_id = '1YLnWNTjsHNhGe65tKdrU_8VRqsAVQmPy4hbqYWpWltw'\n",
    "    sh = gc.open('NLP Sentiment dashboard')\n",
    "    #chooses first sheet to upload to\n",
    "    wks = sh[0]\n",
    "    print(\"df uploaded\")\n",
    "    #clears worksheet then populates A,\n",
    "    if(loc == 1):\n",
    "        wks.clear()\n",
    "        wks.set_dataframe(df,(1,loc))\n",
    "    if(loc > 1):\n",
    "        wks.set_dataframe(df,(1,loc))\n",
    "#(x,y) => 1,1 starts populating B from right to left\n",
    "#(1,2) starts populating b, c\n",
    "\n",
    "topic_extract()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = extract_tweets()\n",
    "# search_term = \"#climate+change -filter:retweets\"\n",
    "# opt = \"y\"\n",
    "# date = '2018-11-01'\n",
    "# num = 1000\n",
    "\n",
    "# df = extract_tweets_configed(search_term, opt, num, date)\n",
    "# df = df[df.polarity != 0]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # #Here for what expected graph should look like in google sheets\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# # Plot histogram with break at zero\n",
    "# df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1],\n",
    "#              ax=ax,\n",
    "#              color=\"purple\")\n",
    "\n",
    "\n",
    "# plt.title(\"Sentiments from Tweets on Climate Change\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spreadsheet_id = '1YLnWNTjsHNhGe65tKdrU_8VRqsAVQmPy4hbqYWpWltw'\n",
    "sh = gc.open('NLP Sentiment dashboard')\n",
    "wks = sh[0]\n",
    "#So graph in next columns/some data is maintained.\n",
    "wks.clear(end='B') #rn for two columns\n",
    "\n",
    "# cell_form = cell('A1')\n",
    "# cell_form = (0,0,0,0)\n",
    "#(x,y) => 1,1 starts populating B from right to left\n",
    "#(1,2) starts populating b, c\n",
    "#sets starting point of population\n",
    "wks.set_dataframe(df,(1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
