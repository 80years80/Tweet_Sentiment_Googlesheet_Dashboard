{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libaries\n",
    "### Modules needed for NLP\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from textblob import TextBlob\n",
    "#Kaggle API modules\n",
    "import subprocess\n",
    "import glob\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorize API's\n",
    "#Authorize Twitter API\n",
    "f = open('/home/pi/twitter_api_creds.json')\n",
    "creds = json.load(f)\n",
    "consumer_key = creds['consumer_key']\n",
    "consumer_secret = creds['consumer_secret']\n",
    "access_token = creds['access_token']\n",
    "access_token_secret = creds['access_token_secret']\n",
    "f.close()\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "#Authorize Kaggle's API\n",
    "kapi = KaggleApi()\n",
    "kapi.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter Functions\n",
    "#Calls on clean() for text cleanup then removes the URL\n",
    "def remove_url(txt):\n",
    "    #Call on clean to clean text first\n",
    "    txt = clean(txt)\n",
    "    #removes URL\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "#Cleans up the text like newlines before url is removed.\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter functions\n",
    "#Returns DF of list of cleaned tweets for sentiment analysis\n",
    "#Calls on clean_url() in above block to clean and remove URL\n",
    "#Takes in user inputs as parameters to search for tweets\n",
    "#Can also be used alone if the terms are hardcoded; for testing\n",
    "def extract_tweets_configed(search_term, opt, num, date_ans, lower = False):\n",
    "    tweets = tw.Cursor(api.search,\n",
    "                   q=search_term,\n",
    "                   lang=\"en\",\n",
    "                   since=date_ans).items(num)\n",
    "    # Remove URLs by calling on functions in block above\n",
    "    tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]\n",
    "    # Create textblob objects of the tweets\n",
    "    sentiment_objects = [TextBlob(tweet) for tweet in tweets_no_urls]\n",
    "    # Get sentiment values\n",
    "    sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "    #put into dataframe\n",
    "    sentiment_df = pd.DataFrame(sentiment_values, index = None, columns=[\"polarity\", \"tweet\"])\n",
    "    #remove neutral sentiment values\n",
    "    #sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "    return sentiment_df\n",
    "#Gets the user inputs to pass to extract_tweets_configed\n",
    "#Prompts for search term, retweet filter option, num to extract and date since.\n",
    "def user_inputs():\n",
    "    # Valid final string ex. \"#climate+change -filter:retweets\"\n",
    "    print(\"Enter tweet search term(s): \")\n",
    "    print(\"E.g. ''#climatechange' should be entered as: #climate+change\")\n",
    "    search_term = input()\n",
    "    #Prompt for retweet filter\n",
    "    opt = input(\"Filter out retweets? Enter as y/n: \")\n",
    "    if (opt == 'y'):\n",
    "        search_term = search_term + \" -filter:retweets\"\n",
    "    #prompt for number of tweets to be extracted    \n",
    "    num = input(\"How many tweets to be extracted? Enter as int: \")\n",
    "    num = int(num)\n",
    "    #prompt for date since in YYYY-MM-DD or default to '2018-11-01'\n",
    "    date = '2018-11-01'\n",
    "    date_ans = input(\"Since what date? Enter as: YYYY-MM-DD or 'n' for default of 2018-11-01: \")\n",
    "    if (date_ans == 'n'):\n",
    "        date_ans = date\n",
    "    df = extract_tweets_configed(search_term, opt, num, date_ans)\n",
    "    return df\n",
    "#Calls on user_inputs and returns a cleaned dataframe of tweets ready for sentiment analysis.\n",
    "def extract_tweets():\n",
    "    return user_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter tweet search term(s): \n",
      "E.g. ''#climatechange' should be entered as: #climate+change\n",
      "#climate+change\n",
      "Filter out retweets? Enter as y/n: y\n",
      "How many tweets to be extracted? Enter as int: 1000\n",
      "Since what date? Enter as: YYYY-MM-DD or 'n' for default of 2018-11-01: n\n"
     ]
    }
   ],
   "source": [
    "df = extract_tweets()\n",
    "\n",
    "# search_term = \"#climate+change -filter:retweets\"\n",
    "# opt = \"y\"\n",
    "# num = 100\n",
    "# date = \"2018-11-01\"\n",
    "# df = extract_tweets_configed(search_term, opt, num, date)\n",
    "\n",
    "df = df[df.polarity != 0]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter directory path where datasets will be locally: /home/pi/kaggle_data_analysis_project/upload_stage\n",
      "Enter file name, do NOT include .csv: out\n"
     ]
    }
   ],
   "source": [
    "#Update this to be full process to be scaled\n",
    "#Save df as a csv locally for later upload to kaggle\n",
    "# loc = '/home/pi/kaggle_data_analysis_project/upload_stage'\n",
    "# name = '/home/pi/kaggle_data_analysis_project/upload_stage/out.csv'\n",
    "loc = input(\"Enter directory path where datasets will be locally: \")\n",
    "name = input(\"Enter file name, do NOT include .csv: \")\n",
    "name = loc + '/' + name + '.csv'\n",
    "df.to_csv(name, index = False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/6.61k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file out.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.61k/6.61k [00:02<00:00, 3.04kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: out.csv (7KB)\n"
     ]
    }
   ],
   "source": [
    "# Setting up checks for needing to init, create,\n",
    "# or simply update a version\n",
    "search_for_init = 'dataset-metadata.json'\n",
    "public = False\n",
    "owd = '/home/pi/kaggle_data_analysis_project'\n",
    "os.chdir(loc)\n",
    "meta_exists = glob.glob('./*.json')\n",
    "\n",
    "if(len(meta_exists) == 0): #if true no json exists\n",
    "    kapi.dataset_initialize(loc)\n",
    "    print(\"Set up initialization .JSON in directory to proceed.\")\n",
    "    \n",
    "#if status is 'ready' then data set exists\n",
    "#if status is None, then create a new dataset\n",
    "status = kapi.dataset_status(\"emrejakbulut/twitter-sentiment-dataset\")\n",
    "if(status == None):\n",
    "    print(\"Check dataset name, if trying to update existing and rerun this block \")\n",
    "    pub = input(\"Make dataset(s) public? Enter y/n: \")\n",
    "    if(pub == 'y'):\n",
    "        public = True\n",
    "    kapi.dataset_create_new(loc, public=public)\n",
    "    \n",
    "if(status == 'ready'):\n",
    "    #keeps old versions by default\n",
    "    kapi.dataset_create_version(loc, \"updated\", delete_old_versions = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file out.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53.5k/53.5k [00:02<00:00, 24.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: out.csv (53KB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "https://www.kaggle.com/emrejakbulut/twitter-sentiment-dataset"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload to kaggle\n",
    "# Initialize the directory, only once\n",
    "#kapi.dataset_initialize(loc) #add in logic using ls *.json\n",
    "#Use this command when making first version\n",
    "# kapi.dataset_create_new(loc)\n",
    "#kapi.dataset_create_version(loc, \"updated\", delete_old_versions = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = input()\n",
    "type(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
